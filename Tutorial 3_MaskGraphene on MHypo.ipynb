{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Integrate MHypo data with MaskGraphene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to use MG to integrate MHypo data. As an example, we analyze the -0.04 / -0.09 sample pair of the dorsolateral prefrontal cortex (DLPFC) dataset. \n",
    "\n",
    "We acquired the data from the spatialLIBD webpage, including manual annotations. Before running the model, please download the input data via [zenodo link](https://zenodo.org/records/10698909)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "from utils import (\n",
    "    build_args_ST,\n",
    "    create_optimizer\n",
    ")\n",
    "from datasets.st_loading_utils import visualization_umap_spatial, create_dictionary_mnn\n",
    "from models import build_model_ST\n",
    "import os\n",
    "import scanpy as sc\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = build_args_ST()\n",
    "args.max_epoch=2000\n",
    "args.max_epoch_triplet=500\n",
    "args.section_ids=[\"-0.04\",\"-0.09\"]\n",
    "args.num_class=8\n",
    "args.num_hidden=\"512,32\"\n",
    "args.alpha_l=1\n",
    "args.lam=1 \n",
    "args.loss_fn=\"sce\" \n",
    "args.mask_rate=0.5 \n",
    "args.in_drop=0 \n",
    "args.attn_drop=0 \n",
    "args.remask_rate=0.1\n",
    "args.seeds=[2023] \n",
    "args.num_remasking=1 \n",
    "args.hvgs=3000 \n",
    "args.dataset=\"MHypo\" \n",
    "args.consecutive_prior=1 \n",
    "args.lr=0.001\n",
    "\n",
    "#### remember to change these paths to your data path/link path\n",
    "args.st_data_dir=\"/home/yunfei/spatial_benchmarking/benchmarking_data/mHypothalamus\"\n",
    "args.pi_dir=\"/home/yunfei/spatial_dl_integration/MaskGraphene/PI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MG training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### consecutive DLPFC slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import scipy\n",
    "import anndata\n",
    "from datasets.st_loading_utils import load_DLPFC\n",
    "from datasets.data_proc import Cal_Spatial_Net, simple_impute\n",
    "\n",
    "def dlpfc_loader(dataset_name, pi, section_ids=[\"151507\", \"151508\"], hvgs=5000, st_data_dir=\"./\"):\n",
    "    \n",
    "    if \"DLPFC\" in dataset_name:\n",
    "        # ad_list = []\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        # delist = []\n",
    "        \n",
    "        for section_id in section_ids:\n",
    "            ad_ = load_DLPFC(root_dir=st_data_dir, section_id=section_id)\n",
    "            ad_.var_names_make_unique(join=\"++\")\n",
    "\n",
    "            # \"\"\"add cached de list\"\"\"\n",
    "            # file_path = \"/home/yunfei/spatial_dl_integration/MaskGraphene/de_temp/\"+section_id+\"_de_list.txt\"\n",
    "            # with open(file_path, 'r') as file:\n",
    "            #     lines = file.readlines()\n",
    "            # de_ = [line.strip() for line in lines]\n",
    "            # delist.append(de_)\n",
    "            # make spot name unique\n",
    "            ad_.obs_names = [x+'_'+section_id for x in ad_.obs_names]\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            Cal_Spatial_Net(ad_, rad_cutoff=150) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            adj_list.append(ad_.uns['adj'])\n",
    "            Batch_list.append(ad_)\n",
    "        \n",
    "        # print(np.nonzero())\n",
    "        ad1, ad2 = simple_impute(Batch_list[0], Batch_list[1], pi)\n",
    "        Batch_list = [ad1, ad2]\n",
    "        Batch_list_new = []\n",
    "        for ad_ in Batch_list:\n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(ad_, flavor=\"seurat_v3\", n_top_genes=hvgs)\n",
    "            sc.pp.normalize_total(ad_, target_sum=1e4)\n",
    "            sc.pp.log1p(ad_)\n",
    "            ad_ = ad_[:, ad_.var['highly_variable']]\n",
    "\n",
    "            # union_list = set(item for sublist in delist for item in sublist)\n",
    "            # ad_ = ad_[:, list(union_list)]\n",
    "            Batch_list_new.append(ad_)\n",
    "\n",
    "            \n",
    "        adata_concat = anndata.concat(Batch_list_new, label=\"slice_name\", keys=section_ids, uns_merge=\"same\")\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "\n",
    "        if pi is not None:\n",
    "            assert adj_concat.shape[0] == pi.shape[0] + pi.shape[1], \"adj matrix shape is not consistent with the pi matrix\"\n",
    "\n",
    "            \"\"\"keep max\"\"\"\n",
    "            # max_values = np.max(pi, axis=1)\n",
    "\n",
    "            # # Create a new array with zero\n",
    "            # pi_keep_argmax = np.zeros_like(pi)\n",
    "\n",
    "            # # Loop through each row and set the maximum value to 1 (or any other desired value)\n",
    "            # for i in range(pi.shape[0]):\n",
    "            #     pi_keep_argmax[i, np.argmax(pi[i])] = max_values[i]\n",
    "            \n",
    "            # pi = pi_keep_argmax\n",
    "            \"\"\"\"\"\"\n",
    "\n",
    "            for i in range(pi.shape[0]):\n",
    "                for j in range(pi.shape[1]):\n",
    "                    if pi[i][j] > 0:\n",
    "                        adj_concat[i][j+pi.shape[0]] = 1\n",
    "                        adj_concat[j+pi.shape[0]][i] = 1\n",
    "        \n",
    "        edgeList = np.nonzero(adj_concat)\n",
    "        graph = dgl.graph((edgeList[0], edgeList[1]))\n",
    "        graph.ndata[\"feat\"] = torch.tensor(adata_concat.X)\n",
    "    num_features = graph.ndata[\"feat\"].shape[1]\n",
    "    return graph, num_features, adata_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Calculating spatial graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell1'] = Spatial_Net['Cell1'].map(id_cell_trans)\n",
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell2'] = Spatial_Net['Cell2'].map(id_cell_trans)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph contains 24762 edges, 4221 cells.\n",
      "5.8664 neighbors per cell on average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Calculating spatial graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell1'] = Spatial_Net['Cell1'].map(id_cell_trans)\n",
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell2'] = Spatial_Net['Cell2'].map(id_cell_trans)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph contains 25692 edges, 4381 cells.\n",
      "5.8644 neighbors per cell on average.\n",
      "=== Use sce_loss and alpha_l=1 ===\n",
      "num_encoder_params: 1414752, num_decoder_params: 1422840, num_params_in_total: 2875746\n",
      "PreModel(\n",
      "  (encoder): GAT(\n",
      "    (gat_layers): ModuleList(\n",
      "      (0): GATConv(\n",
      "        (fc): Linear(in_features=2728, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): GATConv(\n",
      "        (fc): Linear(in_features=512, out_features=32, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (decoder): GAT(\n",
      "    (gat_layers): ModuleList(\n",
      "      (0): GATConv(\n",
      "        (fc): Linear(in_features=32, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): GATConv(\n",
      "        (fc): Linear(in_features=512, out_features=2728, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (encoder_to_decoder): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=32, bias=True)\n",
      "  )\n",
      "  (projector_ema): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=32, bias=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): PReLU(num_parameters=1)\n",
      "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "  )\n",
      "  (encoder_ema): GAT(\n",
      "    (gat_layers): ModuleList(\n",
      "      (0): GATConv(\n",
      "        (fc): Linear(in_features=2728, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): GATConv(\n",
      "        (fc): Linear(in_features=512, out_features=32, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (head): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"file save path\"\"\"\n",
    "exp_fig_dir = args.exp_fig_dir\n",
    "st_data_dir = args.st_data_dir\n",
    "pi_dir= os.path.join(args.pi_dir, args.dataset+'_'.join(args.section_ids))\n",
    "\n",
    "\n",
    "file = open(os.path.join(pi_dir, \"S.pickle\"),'rb') \n",
    "global_PI = pickle.load(file)\n",
    "global_PI = global_PI.toarray()\n",
    "\"\"\"\n",
    "STAGE 1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"train with MSSL\"\"\"\n",
    "graph, num_features, ad_concat = dlpfc_loader(dataset_name=args.dataset, pi=global_PI, section_ids=args.section_ids, hvgs=args.hvgs, st_data_dir=st_data_dir)\n",
    "args.num_features = num_features\n",
    "x = graph.ndata[\"feat\"]\n",
    "\n",
    "model = build_model_ST(args)\n",
    "print(model)\n",
    "\n",
    "device = args.device if args.device >= 0 else \"cpu\"\n",
    "optimizer = create_optimizer(args.optimizer, model, args.lr, args.weight_decay)\n",
    "\n",
    "if args.scheduler:\n",
    "    logging.info(\"Use scheduler\")\n",
    "    scheduler = lambda epoch :( 1 + np.cos((epoch) * np.pi / args.max_epoch) ) * 0.5\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "else:\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training with masked reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:50<00:00, 18.18it/s]\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "graph = graph.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "target_nodes = torch.arange(x.shape[0], device=x.device, dtype=torch.long)\n",
    "epoch_iter = tqdm(range(args.max_epoch))\n",
    "\n",
    "for epoch in epoch_iter:\n",
    "    model.train()\n",
    "    loss = model(graph, x, targets=target_nodes)\n",
    "\n",
    "    loss_dict = {\"loss\": loss.item()}\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.embed(graph, x)\n",
    "    # print(z)\n",
    "\n",
    "# model.eval()\n",
    "ad_concat.obsm[\"MG\"] = z.cpu().detach().numpy()\n",
    "\n",
    "# ari_1_pre = []\n",
    "# ari_2_pre = []\n",
    "# ari_ = visualization_umap_spatial(ad_temp=ad_concat, section_ids=args.section_ids, exp_fig_dir=exp_fig_dir, dataset_name=args.dataset_name, num_iter=\"1\", identifier=\"stage1\", num_class=args.num_class, use_key=\"MG\")\n",
    "# ari_1_pre.append(ari_[0])\n",
    "# ari_2_pre.append(ari_[1])\n",
    "\n",
    "# print(args.section_ids[0], ', ARI = %01.3f' % ari_[0])\n",
    "# print(args.section_ids[1], ', ARI = %01.3f' % ari_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training with triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing datasets (0, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Epoch 1643: train_loss: 0.2435:  82%|████████▏ | 1643/2000 [01:40<00:21, 16.54it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"train with MSSL + triplet loss\"\"\"\n",
    "# logging.info(\"Keep training Model with cse + triplet loss \")\n",
    "\n",
    "graph = graph.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "target_nodes = torch.arange(x.shape[0], device=x.device, dtype=torch.long)\n",
    "epoch_iter = tqdm(range(args.max_epoch))\n",
    "section_ids = np.array(ad_concat.obs['batch_name'].unique())\n",
    "\n",
    "\"\"\"a list of precomputed pi, it has the same length as the iter_comb\"\"\"\n",
    "# mnn_dict = create_dictionary_otn(adata_concat_, pis_list_, section_ids, batch_name='batch_name', conf_thres = 0.0, mode=\"normal\", verbose = 1, iter_comb=iter_comb)\n",
    "mnn_dict = create_dictionary_mnn(ad_concat, use_rep=\"MG\", batch_name='batch_name', k=50, iter_comb=None)\n",
    "anchor_ind = []\n",
    "positive_ind = []\n",
    "negative_ind = []\n",
    "for batch_pair in mnn_dict.keys():  # pairwise compare for multiple batches\n",
    "    batchname_list = ad_concat.obs['batch_name'][mnn_dict[batch_pair].keys()]\n",
    "    #             print(\"before add KNN pairs, len(mnn_dict[batch_pair]):\",\n",
    "    #                   sum(adata_new.obs['batch_name'].isin(batchname_list.unique())), len(mnn_dict[batch_pair]))\n",
    "\n",
    "    cellname_by_batch_dict = dict()\n",
    "    for batch_id in range(len(section_ids)):\n",
    "        cellname_by_batch_dict[section_ids[batch_id]] = ad_concat.obs_names[\n",
    "            ad_concat.obs['batch_name'] == section_ids[batch_id]].values\n",
    "\n",
    "    anchor_list = []\n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    for anchor in mnn_dict[batch_pair].keys():\n",
    "        anchor_list.append(anchor)\n",
    "        ## np.random.choice(mnn_dict[batch_pair][anchor])\n",
    "        positive_spot = mnn_dict[batch_pair][anchor][0]  # select the first positive spot\n",
    "        positive_list.append(positive_spot)\n",
    "        section_size = len(cellname_by_batch_dict[batchname_list[anchor]])\n",
    "        negative_list.append(\n",
    "            cellname_by_batch_dict[batchname_list[anchor]][np.random.randint(section_size)])\n",
    "\n",
    "    batch_as_dict = dict(zip(list(ad_concat.obs_names), range(0, ad_concat.shape[0])))\n",
    "    anchor_ind = np.append(anchor_ind, list(map(lambda _: batch_as_dict[_], anchor_list)))\n",
    "    positive_ind = np.append(positive_ind, list(map(lambda _: batch_as_dict[_], positive_list)))\n",
    "    negative_ind = np.append(negative_ind, list(map(lambda _: batch_as_dict[_], negative_list)))\n",
    "\n",
    "for epoch in epoch_iter:\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    _loss = model(graph, x, targets=target_nodes)\n",
    "    with torch.no_grad():\n",
    "        z = model.embed(graph, x)\n",
    "\n",
    "    anchor_arr = z[anchor_ind,]\n",
    "    positive_arr = z[positive_ind,]\n",
    "    negative_arr = z[negative_ind,]\n",
    "\n",
    "    triplet_loss = torch.nn.TripletMarginLoss(margin=1, p=2, reduction='mean')\n",
    "    tri_output = triplet_loss(anchor_arr, positive_arr, negative_arr)\n",
    "\n",
    "    loss = _loss + tri_output\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    # loss_dict = {\"loss\": loss.item()}\n",
    "    epoch_iter.set_description(f\"# Epoch {epoch}: train_loss: {loss.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.embed(graph, x)\n",
    "\n",
    "# model.eval()\n",
    "ad_concat.obsm[\"MG_triplet\"] = z.cpu().detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_ = visualization_umap_spatial(ad_temp=ad_concat, section_ids=section_ids, exp_fig_dir=exp_fig_dir, dataset_name=args.dataset_name, num_iter=\"1\", identifier=\"stage2\", num_class=args.num_class, use_key=\"MG_triplet\")\n",
    "# ari_1.append(ari_[0])\n",
    "# ari_2.append(ari_[1])\n",
    "print(section_ids[0], ', ARI = %01.3f' % ari_[0])\n",
    "print(section_ids[1], ', ARI = %01.3f' % ari_[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
