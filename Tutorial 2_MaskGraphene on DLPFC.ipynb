{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Integrate DLPFC data with MaskGraphene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to use MG to integrate DLPFC data. As an example, we analyze the 151507/151508 sample pair of the dorsolateral prefrontal cortex (DLPFC) dataset. \n",
    "\n",
    "We acquired the data from the spatialLIBD webpage, including manual annotations. Before running the model, please download the input data via [zenodo link]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 16:54:33,522 - INFO - Enabling RDKit 2022.09.5 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "from utils import (\n",
    "    build_args_ST,\n",
    "    create_optimizer,\n",
    "    set_random_seed,\n",
    "    get_current_lr,\n",
    ")\n",
    "from datasets.data_proc import load_ST_dataset_hard, load_ST_dataset_hard_erase\n",
    "from datasets.st_loading_utils import create_dictionary_otn, visualization_umap_spatial, create_dictionary_mnn, cal_layer_based_alignment_result, cal_layer_based_alignment_result_mhypo\n",
    "from models import build_model_ST\n",
    "import os\n",
    "import scanpy as sc\n",
    "import sklearn.metrics.pairwise\n",
    "\n",
    "# from maskgraphene_main import main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = build_args_ST()\n",
    "args.max_epoch=2000\n",
    "args.max_epoch_triplet=500\n",
    "args.section_ids=[\"151507\",\"151508\"]\n",
    "args.num_class=7\n",
    "args.num_hidden=\"512,32\"\n",
    "args.alpha_l=1\n",
    "args.lam=1 \n",
    "args.loss_fn=\"sce\" \n",
    "args.mask_rate=0.5 \n",
    "args.in_drop=0 \n",
    "args.attn_drop=0 \n",
    "args.remask_rate=0.1\n",
    "args.seeds=[2023] \n",
    "args.num_remasking=1 \n",
    "args.hvgs=3000 \n",
    "args.dataset=\"DLPFC\" \n",
    "args.consecutive_prior=1 \n",
    "args.lr=0.001\n",
    "\n",
    "#### remember to change these paths to your data path/link path\n",
    "args.st_data_dir=\"/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12\"\n",
    "args.pi_dir=\"/home/yunfei/spatial_dl_integration/MaskGraphene/PI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MG training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### consecutive slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import scipy\n",
    "import anndata\n",
    "from datasets.st_loading_utils import load_DLPFC, Cal_Spatial_Net, simple_impute\n",
    "\n",
    "def dlpfc_loader(dataset_name, pi, section_ids=[\"151507\", \"151508\"], hvgs=5000, st_data_dir=\"./\"):\n",
    "    \n",
    "    if \"DLPFC\" in dataset_name:\n",
    "        # ad_list = []\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        # delist = []\n",
    "        \n",
    "        for section_id in section_ids:\n",
    "            ad_ = load_DLPFC(root_dir=st_data_dir, section_id=section_id)\n",
    "            ad_.var_names_make_unique(join=\"++\")\n",
    "\n",
    "            # \"\"\"add cached de list\"\"\"\n",
    "            # file_path = \"/home/yunfei/spatial_dl_integration/MaskGraphene/de_temp/\"+section_id+\"_de_list.txt\"\n",
    "            # with open(file_path, 'r') as file:\n",
    "            #     lines = file.readlines()\n",
    "            # de_ = [line.strip() for line in lines]\n",
    "            # delist.append(de_)\n",
    "            # make spot name unique\n",
    "            ad_.obs_names = [x+'_'+section_id for x in ad_.obs_names]\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            Cal_Spatial_Net(ad_, rad_cutoff=150) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            adj_list.append(ad_.uns['adj'])\n",
    "            Batch_list.append(ad_)\n",
    "        \n",
    "        # print(np.nonzero())\n",
    "        ad1, ad2 = simple_impute(Batch_list[0], Batch_list[1], pi)\n",
    "        Batch_list = [ad1, ad2]\n",
    "        Batch_list_new = []\n",
    "        for ad_ in Batch_list:\n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(ad_, flavor=\"seurat_v3\", n_top_genes=hvgs)\n",
    "            sc.pp.normalize_total(ad_, target_sum=1e4)\n",
    "            sc.pp.log1p(ad_)\n",
    "            ad_ = ad_[:, ad_.var['highly_variable']]\n",
    "\n",
    "            # union_list = set(item for sublist in delist for item in sublist)\n",
    "            # ad_ = ad_[:, list(union_list)]\n",
    "            Batch_list_new.append(ad_)\n",
    "\n",
    "            \n",
    "        adata_concat = anndata.concat(Batch_list_new, label=\"slice_name\", keys=section_ids, uns_merge=\"same\")\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "\n",
    "        if pi is not None:\n",
    "            assert adj_concat.shape[0] == pi.shape[0] + pi.shape[1], \"adj matrix shape is not consistent with the pi matrix\"\n",
    "\n",
    "            \"\"\"keep max\"\"\"\n",
    "            # max_values = np.max(pi, axis=1)\n",
    "\n",
    "            # # Create a new array with zero\n",
    "            # pi_keep_argmax = np.zeros_like(pi)\n",
    "\n",
    "            # # Loop through each row and set the maximum value to 1 (or any other desired value)\n",
    "            # for i in range(pi.shape[0]):\n",
    "            #     pi_keep_argmax[i, np.argmax(pi[i])] = max_values[i]\n",
    "            \n",
    "            # pi = pi_keep_argmax\n",
    "            \"\"\"\"\"\"\n",
    "\n",
    "            for i in range(pi.shape[0]):\n",
    "                for j in range(pi.shape[1]):\n",
    "                    if pi[i][j] > 0:\n",
    "                        adj_concat[i][j+pi.shape[0]] = 1\n",
    "                        adj_concat[j+pi.shape[0]][i] = 1\n",
    "        \n",
    "        edgeList = np.nonzero(adj_concat)\n",
    "        graph = dgl.graph((edgeList[0], edgeList[1]))\n",
    "        graph.ndata[\"feat\"] = torch.tensor(adata_concat.X)\n",
    "    num_features = graph.ndata[\"feat\"].shape[1]\n",
    "    return graph, num_features, adata_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if hard-links provided\"\"\"\n",
    "\n",
    "\"\"\"file save path\"\"\"\n",
    "exp_fig_dir = args.exp_fig_dir\n",
    "st_data_dir = args.st_data_dir\n",
    "pi_dir= os.path.join(args.pi_dir, args.dataset+'_'.join(args.section_ids))\n",
    "\n",
    "\n",
    "file = open(os.path.join(pi_dir, \"S.pickle\"),'rb') \n",
    "global_PI = pickle.load(file)\n",
    "global_PI = global_PI.toarray()\n",
    "\"\"\"\n",
    "STAGE 1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"train with MSSL\"\"\"\n",
    "graph, num_features, ad_concat = dlpfc_loader(dataset_name=args.dataset, pi=global_PI, section_ids=args.section_ids, hvgs=args.hvgs, st_data_dir=st_data_dir)\n",
    "args.num_features = num_features\n",
    "x = graph.ndata[\"feat\"]\n",
    "\n",
    "model = build_model_ST(args)\n",
    "print(model)\n",
    "\n",
    "device = args.device if args.device >= 0 else \"cpu\"\n",
    "optimizer = create_optimizer(args.optimizer, model, args.lr, args.weight_decay)\n",
    "\n",
    "if args.scheduler:\n",
    "    logging.info(\"Use scheduler\")\n",
    "    scheduler = lambda epoch :( 1 + np.cos((epoch) * np.pi / args.max_epoch) ) * 0.5\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "graph = graph.to(device)\n",
    "x = x.to(device)\n",
    "# print(ad_concat[0])\n",
    "model, ad_concat_1 = MG(model, graph, x, optimizer, max_epoch, device, ad_concat, scheduler, logger=logger, key_=\"MG\")\n",
    "# print(ad_concat_1)\n",
    "# print(ad_concat_1.obsm[\"MG\"])\n",
    "ari_ = visualization_umap_spatial(ad_temp=ad_concat_1, section_ids=section_ids, exp_fig_dir=exp_fig_dir, dataset_name=dataset_name, num_iter=counter, identifier=\"stage1\", num_class=args.num_class, use_key=\"MG\")\n",
    "ari_1_pre.append(ari_[0])\n",
    "ari_2_pre.append(ari_[1])\n",
    "if logger is not None:\n",
    "    logger.log({\"slice1_ari_pre\": ari_[0], \"slice2_ari_pre\": ari_[1]})\n",
    "# print(section_id)\n",
    "print(section_ids[0], ', ARI = %01.3f' % ari_[0])\n",
    "print(section_ids[1], ', ARI = %01.3f' % ari_[1])\n",
    "# exit(-1)\n",
    "\"\"\"train with MSSL + triplet loss\"\"\"\n",
    "logging.info(\"Keep training Model with cse + triplet loss \")\n",
    "\n",
    "model, ad_concat_2 = MG_triplet(model, graph, x, optimizer, max_epoch_triplet, device, adata_concat_=ad_concat_1, pis_list_=[global_PI], scheduler=scheduler, logger=logger, key_=\"MG_triplet\")\n",
    "ari_ = visualization_umap_spatial(ad_temp=ad_concat_2, section_ids=section_ids, exp_fig_dir=exp_fig_dir, dataset_name=dataset_name, num_iter=counter, identifier=\"stage2\", num_class=args.num_class, use_key=\"MG_triplet\")\n",
    "counter += 1\n",
    "ari_1.append(ari_[0])\n",
    "ari_2.append(ari_[1])\n",
    "if logger is not None:\n",
    "    logger.log({\"slice1_ari_after\": ari_[0], \"slice2_ari_after\": ari_[1]})\n",
    "print(section_ids[0], ', ARI = %01.3f' % ari_[0])\n",
    "print(section_ids[1], ', ARI = %01.3f' % ari_[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
