{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Generate hard-links for consecutive ST slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from utils import (\n",
    "    build_args_ST,\n",
    "    create_optimizer\n",
    ")\n",
    "\n",
    "from models import build_model_ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = build_args_ST()\n",
    "args.max_epoch=2000\n",
    "args.max_epoch_triplet=500\n",
    "args.dataset_name=\"DLPFC\"\n",
    "args.section_ids=[\"151507\", \"151508\"]\n",
    "args.num_class=7\n",
    "args.num_hidden=\"512,32\"\n",
    "args.alpha_l=1\n",
    "args.lam=1 \n",
    "args.loss_fn=\"sce\" \n",
    "args.mask_rate=0.5 \n",
    "args.in_drop=0 \n",
    "args.attn_drop=0 \n",
    "args.remask_rate=0.1\n",
    "args.seeds=[2023] \n",
    "args.num_remasking=1 \n",
    "args.hvgs=3000 \n",
    "args.dataset=\"DLPFC\" \n",
    "args.consecutive_prior=1 \n",
    "args.lr=0.001\n",
    "args.scheduler = True\n",
    "\n",
    "#### remember to change this to your data path\n",
    "args.st_data_dir=\"/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(seeds=[2023], dataset='DLPFC', exp_fig_dir='./', h5ad_save_dir='./', st_data_dir='path/to/stdata', pi_dir='./', consecutive_prior=1, section_ids='151507,151508', num_class=7, hvgs=3000, device=3, max_epoch=2000, max_epoch_triplet=500, warmup_steps=-1, num_heads=1, num_out_heads=1, num_layers=2, num_dec_layers=2, num_remasking=1, num_hidden='512,32', residual=False, in_drop=0, attn_drop=0, norm=None, lr=0.001, weight_decay=0, negative_slope=0.2, activation='elu', mask_rate=0.5, remask_rate=0.1, remask_method='random', mask_type='mask', mask_method='random', drop_edge_rate=0.0, encoder='gat', decoder='gat', loss_fn='sce', alpha_l=1, optimizer='adam', linear_prob=False, no_pretrain=False, checkpoint_path=None, use_cfg=False, logging=False, scheduler=False, batch_size=512, sampling_method='saint', label_rate=1.0, lam=1, delayed_ema_epoch=0, replace_rate=0.0, momentum=0.996, load_model=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 15:51:30,053 - INFO - Enabling RDKit 2022.09.5 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from datasets.st_loading_utils import load_DLPFC, create_dictionary_mnn, load_mHypothalamus\n",
    "from datasets.data_proc import Cal_Spatial_Net\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "\n",
    "def localOT_loader(section_ids=[\"151507\", \"151508\"], dataname=\"DLPFC\", hvgs=5000, st_data_dir=\"./\", hard_links=None):\n",
    "    # hard links is a mapping matrix (2d numpy array with the size of #slice1 spot by #slice2 spot)\n",
    "    if dataname == \"DLPFC\":\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        for section_id in section_ids:\n",
    "            ad_ = load_DLPFC(root_dir=st_data_dir, section_id=section_id)\n",
    "            ad_.var_names_make_unique(join=\"++\")\n",
    "        \n",
    "            # make spot name unique\n",
    "            ad_.obs_names = [x+'_'+section_id for x in ad_.obs_names]\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            Cal_Spatial_Net(ad_, rad_cutoff=150) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            \n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(ad_, flavor=\"seurat_v3\", n_top_genes=hvgs)\n",
    "            sc.pp.normalize_total(ad_, target_sum=1e4)\n",
    "            sc.pp.log1p(ad_)\n",
    "            ad_ = ad_[:, ad_.var['highly_variable']]\n",
    "\n",
    "            adj_list.append(ad_.uns['adj'])\n",
    "            Batch_list.append(ad_)\n",
    "        adata_concat = anndata.concat(Batch_list, label=\"slice_name\", keys=section_ids, uns_merge=\"same\")\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "        \n",
    "        \"\"\"if hard links is not empty\"\"\"\n",
    "        if hard_links != None:\n",
    "            for i in range(hard_links.shape[0]):\n",
    "                for j in range(hard_links.shape[1]):\n",
    "                    if hard_links[i][j] > 0:\n",
    "                        adj_concat[i][j+hard_links.shape[0]] = 1\n",
    "                        adj_concat[j+hard_links.shape[0]][i] = 1\n",
    "\n",
    "        edgeList = np.nonzero(adj_concat)\n",
    "        graph = dgl.graph((edgeList[0], edgeList[1]))\n",
    "        graph.ndata[\"feat\"] = torch.tensor(adata_concat.X.todense())\n",
    "        num_features = graph.ndata[\"feat\"].shape[1]\n",
    "    elif dataname == \"mHypothalamus\":\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        for section_id in section_ids:\n",
    "            ad_ = load_mHypothalamus(root_dir=st_data_dir, section_id=section_id)\n",
    "            ad_.var_names_make_unique(join=\"++\")\n",
    "        \n",
    "            # make spot name unique\n",
    "            ad_.obs_names = [x+'_'+section_id for x in ad_.obs_names]\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            Cal_Spatial_Net(ad_, rad_cutoff=35) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            \n",
    "            # Normalization\n",
    "            sc.pp.normalize_total(ad_, target_sum=1e4)\n",
    "            sc.pp.log1p(ad_)\n",
    "\n",
    "            adj_list.append(ad_.uns['adj'])\n",
    "            Batch_list.append(ad_)\n",
    "        adata_concat = anndata.concat(Batch_list, label=\"slice_name\", keys=section_ids, uns_merge=\"same\")\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "        \n",
    "        \"\"\"if hard links is not empty\"\"\"\n",
    "        if hard_links != None:\n",
    "            for i in range(hard_links.shape[0]):\n",
    "                for j in range(hard_links.shape[1]):\n",
    "                    if hard_links[i][j] > 0:\n",
    "                        adj_concat[i][j+hard_links.shape[0]] = 1\n",
    "                        adj_concat[j+hard_links.shape[0]][i] = 1\n",
    "\n",
    "        edgeList = np.nonzero(adj_concat)\n",
    "        graph = dgl.graph((edgeList[0], edgeList[1]))\n",
    "        graph.ndata[\"feat\"] = torch.tensor(adata_concat.X).float()\n",
    "        num_features = graph.ndata[\"feat\"].shape[1]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return graph, num_features, adata_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Calculating spatial graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell1'] = Spatial_Net['Cell1'].map(id_cell_trans)\n",
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell2'] = Spatial_Net['Cell2'].map(id_cell_trans)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph contains 24762 edges, 4221 cells.\n",
      "5.8664 neighbors per cell on average.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/yunfei/anaconda3/envs/MG/lib/python3.9/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Calculating spatial graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell1'] = Spatial_Net['Cell1'].map(id_cell_trans)\n",
      "/home/yunfei/spatial_dl_integration/MaskGraphene/datasets/data_proc.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Spatial_Net['Cell2'] = Spatial_Net['Cell2'].map(id_cell_trans)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph contains 25692 edges, 4381 cells.\n",
      "5.8644 neighbors per cell on average.\n",
      "=== Use sce_loss and alpha_l=1 ===\n",
      "num_encoder_params: 601696, num_decoder_params: 605020, num_params_in_total: 1243282\n",
      "PreModel(\n",
      "  (encoder): GAT(\n",
      "    (gat_layers): ModuleList(\n",
      "      (0): GATConv(\n",
      "        (fc): Linear(in_features=1140, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): GATConv(\n",
      "        (fc): Linear(in_features=512, out_features=32, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (decoder): GAT(\n",
      "    (gat_layers): ModuleList(\n",
      "      (0): GATConv(\n",
      "        (fc): Linear(in_features=32, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): GATConv(\n",
      "        (fc): Linear(in_features=512, out_features=1140, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (encoder_to_decoder): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=32, bias=True)\n",
      "  )\n",
      "  (projector_ema): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=512, out_features=32, bias=True)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): PReLU(num_parameters=1)\n",
      "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "  )\n",
      "  (encoder_ema): GAT(\n",
      "    (gat_layers): ModuleList(\n",
      "      (0): GATConv(\n",
      "        (fc): Linear(in_features=1140, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        (activation): ELU(alpha=1.0)\n",
      "      )\n",
      "      (1): GATConv(\n",
      "        (fc): Linear(in_features=512, out_features=32, bias=False)\n",
      "        (feat_drop): Dropout(p=0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (head): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "graph, num_features, ad_concat = localOT_loader(section_ids=args.section_ids, hvgs=args.hvgs, st_data_dir=args.st_data_dir, dataname=args.dataset_name)\n",
    "args.num_features = num_features\n",
    "model = build_model_ST(args)\n",
    "print(model)\n",
    "\n",
    "device = args.device if args.device >= 0 else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = create_optimizer(args.optimizer, model, args.lr, args.weight_decay)\n",
    "\n",
    "if args.scheduler:\n",
    "    scheduler = lambda epoch :( 1 + np.cos((epoch) * np.pi / args.max_epoch) ) * 0.5\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "else:\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masked reconstruction loss training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training local clusters ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Epoch 1999: train_loss: 0.2209: 100%|██████████| 2000/2000 [01:17<00:00, 25.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "x = graph.ndata[\"feat\"]\n",
    "model.to(device)\n",
    "graph = graph.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "\"\"\"training\"\"\"\n",
    "target_nodes = torch.arange(x.shape[0], device=x.device, dtype=torch.long)\n",
    "print(args.max_epoch)\n",
    "epoch_iter = tqdm(range(args.max_epoch))\n",
    "\n",
    "print(\"training local clusters ... \")\n",
    "for epoch in epoch_iter:\n",
    "    model.train()\n",
    "    # print(type(x), type(graph))\n",
    "    loss = model(graph, x, targets=target_nodes)\n",
    "\n",
    "    loss_dict = {\"loss\": loss.item()}\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_iter.set_description(f\"# Epoch {epoch}: train_loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = model.embed(graph, x)\n",
    "ad_concat.obsm[\"maskgraphene\"] = embedding.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triplet loss training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing datasets (0, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Epoch 499: train_loss: 0.2467: 100%|██████████| 500/500 [00:19<00:00, 26.28it/s]\n",
      "2024-03-06 16:10:32,961 - INFO - cffi mode is CFFI_MODE.ANY\n",
      "2024-03-06 16:10:33,057 - INFO - R home found: /usr/lib/R\n",
      "2024-03-06 16:10:33,273 - INFO - R library path: /usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server\n",
      "2024-03-06 16:10:33,276 - INFO - LD_LIBRARY_PATH: /usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server\n",
      "2024-03-06 16:10:33,286 - INFO - Default options to initialize R: rpy2, --quiet, --no-save\n",
      "2024-03-06 16:10:33,463 - INFO - R is already initialized. No need to initialize.\n",
      "2024-03-06 16:10:33,473 - WARNING - R[write to console]:                    __           __ \n",
      "   ____ ___  _____/ /_  _______/ /_\n",
      "  / __ `__ \\/ ___/ / / / / ___/ __/\n",
      " / / / / / / /__/ / /_/ (__  ) /_  \n",
      "/_/ /_/ /_/\\___/_/\\__,_/____/\\__/   version 5.4.10\n",
      "Type 'citation(\"mclust\")' for citing this R package in publications.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.1615634   -6.23971     -2.3459961  ...  -2.3171792   -8.681177\n",
      "    0.41282746]\n",
      " [ -2.460916   -15.380771     0.09077121 ...  -2.9557252  -20.356768\n",
      "   -3.8703644 ]\n",
      " [ -2.6018689   -3.7312057   -4.6612043  ...  -7.7043953   -6.703478\n",
      "   -1.9260284 ]\n",
      " ...\n",
      " [ -2.2426836  -15.747475    -0.50736713 ...  -4.671777   -26.402672\n",
      "   -3.7827911 ]\n",
      " [ -5.4853687   -0.9318517   -6.884806   ...  -9.694893   -11.444196\n",
      "    1.916019  ]\n",
      " [ -1.3684633   -3.4660091   -2.5994942  ...  -7.151997    -6.307693\n",
      "   -2.191994  ]]\n",
      "fitting ...\n",
      "  |======================================================================| 100%\n",
      "151507\n",
      "mclust, ARI = 0.444\n",
      "151508\n",
      "mclust, ARI = 0.460\n"
     ]
    }
   ],
   "source": [
    "from datasets.st_loading_utils import mclust_R\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "\n",
    "mnn_dict = create_dictionary_mnn(ad_concat, use_rep=\"maskgraphene\", batch_name='batch_name', k=50, verbose = 1, iter_comb=None)\n",
    "\n",
    "anchor_ind = []\n",
    "positive_ind = []\n",
    "negative_ind = []\n",
    "for batch_pair in mnn_dict.keys():  # pairwise compare for multiple batches\n",
    "    batchname_list = ad_concat.obs['batch_name'][mnn_dict[batch_pair].keys()]\n",
    "\n",
    "    cellname_by_batch_dict = dict()\n",
    "    for batch_id in range(len(args.section_ids)):\n",
    "        cellname_by_batch_dict[args.section_ids[batch_id]] = ad_concat.obs_names[\n",
    "            ad_concat.obs['batch_name'] == args.section_ids[batch_id]].values\n",
    "\n",
    "    anchor_list = []\n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    for anchor in mnn_dict[batch_pair].keys():\n",
    "        anchor_list.append(anchor)\n",
    "        ## np.random.choice(mnn_dict[batch_pair][anchor])\n",
    "        positive_spot = mnn_dict[batch_pair][anchor][0]  # select the first positive spot\n",
    "        positive_list.append(positive_spot)\n",
    "        section_size = len(cellname_by_batch_dict[batchname_list[anchor]])\n",
    "        negative_list.append(\n",
    "            cellname_by_batch_dict[batchname_list[anchor]][np.random.randint(section_size)])\n",
    "\n",
    "    batch_as_dict = dict(zip(list(ad_concat.obs_names), range(0, ad_concat.shape[0])))\n",
    "    anchor_ind = np.append(anchor_ind, list(map(lambda _: batch_as_dict[_], anchor_list)))\n",
    "    positive_ind = np.append(positive_ind, list(map(lambda _: batch_as_dict[_], positive_list)))\n",
    "    negative_ind = np.append(negative_ind, list(map(lambda _: batch_as_dict[_], negative_list)))\n",
    "\n",
    "epoch_iter = tqdm(range(args.max_epoch_triplet))\n",
    "for epoch in epoch_iter:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    _loss = model(graph, x, targets=target_nodes)\n",
    "    if epoch % 100 == 0 or epoch == 500:\n",
    "        with torch.no_grad():\n",
    "            z = model.embed(graph, x)\n",
    "        \n",
    "        anchor_arr = z[anchor_ind,]\n",
    "        positive_arr = z[positive_ind,]\n",
    "        negative_arr = z[negative_ind,]\n",
    "\n",
    "    triplet_loss = torch.nn.TripletMarginLoss(margin=1, p=2, reduction='mean')\n",
    "    tri_output = triplet_loss(anchor_arr, positive_arr, negative_arr)\n",
    "\n",
    "    loss = _loss + tri_output\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "    optimizer.step()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    loss_dict = {\"loss\": loss.item()}\n",
    "    epoch_iter.set_description(f\"# Epoch {epoch}: train_loss: {loss.item():.4f}\")\n",
    "\n",
    "    # z = model.embed(graph, x)\n",
    "with torch.no_grad():\n",
    "    embedding = model.embed(graph, x)\n",
    "ad_concat.obsm[\"maskgraphene_mnn\"] = embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\"\"\"calculate ARI & umap & viz\"\"\"\n",
    "mclust_R(ad_concat, modelNames='EEE', num_cluster=args.num_class, used_obsm='maskgraphene_mnn')\n",
    "\n",
    "\n",
    "ad_temp = ad_concat[ad_concat.obs['original_clusters']!='unknown']\n",
    "\n",
    "\n",
    "Batch_list = []\n",
    "for section_id in args.section_ids:\n",
    "    ad__ = ad_temp[ad_temp.obs['batch_name'] == section_id]\n",
    "    Batch_list.append(ad__)\n",
    "    print(section_id)\n",
    "    print('mclust, ARI = %01.3f' % ari_score(ad__.obs['original_clusters'], ad__.obs['mclust']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paste alignment to generate hard-links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n",
      "gpu is available, using gpu.\n"
     ]
    }
   ],
   "source": [
    "import paste\n",
    "import ot\n",
    "\n",
    "slice1 = Batch_list[0]\n",
    "slice2 = Batch_list[1]\n",
    "global_PI = np.zeros((len(slice1.obs.index), len(slice2.obs.index)))\n",
    "slice1_idx_mapping = {}\n",
    "slice2_idx_mapping = {}\n",
    "for i in range(len(slice1.obs.index)):\n",
    "    slice1_idx_mapping[slice1.obs.index[i]] = i\n",
    "for i in range(len(slice2.obs.index)):\n",
    "    slice2_idx_mapping[slice2.obs.index[i]] = i\n",
    "\n",
    "# temp_local_pi_list = []\n",
    "for i in range(args.num_class):\n",
    "    subslice1 = slice1[slice1.obs['mclust']==i+1]\n",
    "    subslice2 = slice2[slice2.obs['mclust']==i+1]\n",
    "    if subslice1.shape[0]>0 and subslice2.shape[0]>0:\n",
    "        pi00 = paste.match_spots_using_spatial_heuristic(subslice1.obsm['spatial'], subslice2.obsm['spatial'], use_ot= True)\n",
    "        local_PI = paste.pairwise_align(subslice1, subslice2, alpha=0.1, dissimilarity='kl', use_rep=None, G_init=pi00, use_gpu = True, backend = ot.backend.TorchBackend())\n",
    "        for i in range(local_PI.shape[0]):\n",
    "            for j in range(local_PI.shape[1]):\n",
    "                global_PI[slice1_idx_mapping[subslice1.obs.index[i]]][slice2_idx_mapping[subslice2.obs.index[j]]] = local_PI[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save/load Hard-links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = scipy.sparse.csr_matrix(global_PI)\n",
    "file = open(os.path.join(args.save_pi_dir, \"pi_151507_151508.pickle\"),'wb')\n",
    "pickle.dump(S, file)\n",
    "\n",
    "\"\"\"\n",
    "file = open(\"pi_151507_151508.pickle\",'rb') \n",
    "S = pickle.load(file)\n",
    "S.toarray()\n",
    "\n",
    "to retrieve from saved pi file\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
